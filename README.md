# Image Captioning Using Vision Transformer and GPT-2

This project implements an **image captioning model** using the **COCO dataset**. It combines the power of **Vision Transformers (ViT)** for image feature extraction and **GPT-2** for natural language generation, producing descriptive captions for images.

## Features
- Uses **Vision Transformers (ViT)** for image encoding.
- Employs **GPT-2** for generating natural language captions.
- Trained and evaluated on the **COCO dataset**.
- Achieves robust results with evaluation metrics like **BLEU**, **ROUGE**, and **CIDEr**.

---

### Dattaset used : The COCO Dataset

1. Visit the [COCO Dataset Page](https://cocodataset.org/#download).
2. Download the following files:
   - **Train images**: `train2017.zip`
   - **Validation images**: `val2017.zip`
   - **Annotations**: `annotations_trainval2017.zip`


